\documentclass[11pt,fancy,authoryear]{elegantbook}

\title{CUDA in Detail}
\subtitle{An in-depth guide to GPGPU programming}

\author{Nick Green}
\institute{CoffeeBeforeArch}
\date{December 31, 2020}
\version{0.0}
%\bioinfo{Bio}{Information}

%\extrainfo{Victory won\rq t come to us unless we go to it. }

%\logo{logo-blue.png}
\cover{cover.jpg}

\begin{document}

\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Introduction}

\section{Why This Book on CUDA?}

Out of all resources for learning GPGPU programming with CUDA, why should you pick this one? Here is a list of reasons!

\begin{itemize}
  \item \textbf{It's free:} This is book (and its source) will always be free to download
  \item \textbf{It can be updated:} Printed books can not be updated. With a digital book, I can make modifications as new versions of CUDA are released, or correct errors in the text.
  \item \textbf{It focuses on performance:} Performance is the primary reason you would offload computation to the GPU. As such, it will be the primary focus of each chapter.
\end{itemize}

\section{Intended Audience}

This book was written as a companion piece to my YouTube series \textbf{CUDA Crash Course}. As such, the intended audience is expected to have experience in C/C++ programming, and familiarity with parallel programming concepts (atomic operations, threads, etc.). Additionally, a basic knowledge of computer organization and architecture is assumed (this is necessary for performance tuning).

No backround in GPGPU (or similar accelerator) programming is required.

\section{Environment}

\begin{itemize}
  \item \textbf{Operating System: } Ubuntu 20.04
  \item \textbf{Host Compiler: } GCC 10.2
  \item \textbf{CUDA Toolkit Version: } CUDA 11.0
  \item \textbf{Processor: } Intel x86\-64
  \item \textbf{GPU: } NVIDIA GeForce RTX 2060
\end{itemize}

\section{About the Author}

My name is Nick, and I'm a computer systems architect working on deep learning accelerator performance in the San Francisco Bay Area.

My background in GPGPUs architecture in programming came during my Ph.D. (incomplete). At that time, I worked in a research lab that focused on accelerator architectures, and developed the simulator GPGPU-Sim which modeled NVIDIA GPGPU architectures.

To solidify my knowledge about GPGPU architecture and inspire new ideas for research, I began writing GPGPU applications and studying their performance. These examples layed the foundation for what would become my YouTube series \textbf{CUDA Crash Course}.

Since that time, studying parallel architectures and performance has been one of my passions, and is what I do each day at work.

\chapter{CUDA Basics}

In this first chapter, we will take a look at some of the core parts of CUDA programs.

\begin{introduction}
  \item Grids, Blocks, and Threads
  \item Memory
  \item Calling Kernels
\end{introduction}

\section{Grids, Blocks, and Threads}

Unlike CPU programming which has a largely temporal programming model, the GPU programming model is inherently spatial. The first step in writing a GPU kernel is making a deciding how to spatially decompose your problem across threads.

\section{Memory}

One of the unique aspects of GPU programming is that we are typically working with memory that is physically separate from the CPU (with the exception of some SoCs like the Tegra).

\section{Kernels}

Functions that we call from the CPU and run from the GPU are typically called kernels, and are typically the highly-parallel innermost loops of an algorithm.

In addition to kernels, we have device functions. Device functions are functions that are called from the GPU, and run on the GPU.

\section{Calling Kernels}

The final step in launching a kernel on the GPU is setting up the kernel launch. This is similar to a CPU function call with the exception of the required kernel launch parameters. These include:

\begin{itemize}
  \item The dimensions of threadblocks
  \item The dimensions of the kernel's grid
  \item The amount of shared memory to allocate
  \uten The stream in which to launch the kernels
\end{itemize}

\chapter{Vector Addition}

The classical "Hello, world!" kernel for GPUs is vector addition (often shortened as vector add). In this chapter, we'll look at how to set up both the host (CPU) and device (GPU) code for a program that caclulates the sum of the two vectors.

\chapter{Memory Copies}

One of the most important parts of GPU programming is data movement, and like much of programming, there are many ways to solve a problem. In this chapter, we'll be looking at some of the different ways we can orchestrate data movement between the CPU and GPU.

\begin{introduction}
  \item Unified Memory
  \item Pinned Memory
  \item Zero-Copy
  \item Asynchronous Copies
\end{introduction}

\section{Unified Memory}

\section{Pinned Memory}

\section{Zero-Copy}

\section{Asynchronous Copies}

\chapter{Matrix Multiplication}

\section{Baseline}

\section{Shared Memory}

\chapter{Sum Reduction}

\section{Baseline}

\section{Sequential Thread Composition}

\section{Sequential Addressing}

\section{Work Packing}

\section{Loop Unrolling}

\section{Further Work Parking}

\chapter{Histograms}

\section{Global Atomics}

\section{Shared Memory Atomics}

\chapter{Convolution}

\section{1-D Convolution}

\section{2-D Convolution}

\section{3-D Convolution}

\end{document}
